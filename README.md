# CRAWL TOPCV JOBS (Learning Purposes)

Crawler nháº¹ nhÃ ng Ä‘á»ƒ thu tháº­p tin tuyá»ƒn dá»¥ng **Data Analyst** trÃªn TopCV, tá»± Ä‘á»™ng ghÃ©p **chi tiáº¿t job** + **thÃ´ng tin cÃ´ng ty** vÃ o má»™t **Pandas DataFrame**, xuáº¥t **CSV/XLSX**.

## âœ… TÃ­nh nÄƒng

* Crawl tá»« trang search (Ä‘a trang).
* VÃ o tá»«ng `job_url` láº¥y: má»©c lÆ°Æ¡ng, Ä‘á»‹a Ä‘iá»ƒm, kinh nghiá»‡m, deadline, mÃ´ táº£/yÃªu cáº§u/quyá»n lá»£i, tag, Ä‘á»‹a chá»‰ & thá»i gian lÃ m viá»‡c.
* VÃ o `company_url` (náº¿u cÃ³) láº¥y: tÃªn, website, quy mÃ´, lÄ©nh vá»±c, Ä‘á»‹a chá»‰, mÃ´ táº£.
* Chá»‘ng **429 Too Many Requests** báº±ng delay ngáº«u nhiÃªn (nháº¹ nhÃ ng).
* Xuáº¥t **CSV** (vÃ  dá»… má»Ÿ rá»™ng sang XLSX).

---

## ğŸ§± Cáº¥u trÃºc project

```
.
â”œâ”€â”€ airflow
â”‚   â”œâ”€â”€ dags
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ scrape_topcv_company.py # Script chÃ­nh
â”‚   â””â”€â”€ topcv.ipynb # Notebook thá»­ nghiá»‡m
â”œâ”€â”€ data-files
â”‚   â”œâ”€â”€ topcv_data_analyst_jobs.csv  # Káº¿t quáº£ CSV (vÃ­ dá»¥)
â”‚   â””â”€â”€ topcv_data_analyst_jobs.xlsx # Káº¿t quáº£ XLSX (tuá»³ chá»n)
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ uv.lock
```

---

## ğŸ› ï¸ YÃªu cáº§u

* Python 3.8+
* [uv](https://github.com/astral-sh/uv) (quáº£n lÃ½ mÃ´i trÆ°á»ng & dependency)
* (Tuá»³ chá»n) Google Chrome + ChromeDriver náº¿u báº¡n dÃ¹ng Selenium thay vÃ¬ `requests`/`BeautifulSoup`.

---

## ğŸš€ Báº¯t Ä‘áº§u

### BÆ°á»›c 1 â€” Khá»Ÿi táº¡o dá»± Ã¡n vá»›i **uv**

```bash
uv init crawl-topcv-jobs
cd crawl-topcv-jobs
```

### BÆ°á»›c 2 â€” CÃ i thÆ° viá»‡n

> Script chÃ­nh dÃ¹ng `requests`, `beautifulsoup4`, `lxml`, `pandas`.
> Báº¡n cÃ³ thá»ƒ cÃ i thÃªm cÃ¡c lib phá»¥c vá»¥ phÃ¢n tÃ­ch (matplotlib, seaborn) hoáº·c Selenium náº¿u cáº§n.

```bash
# Core crawl + phÃ¢n tÃ­ch
uv add requests beautifulsoup4 lxml pandas

# Tuá»³ chá»n (phÃ¢n tÃ­ch/plot, notebook, selenium)
uv add matplotlib seaborn ipykernel selenium webdriver-manager
```

### (Tuá»³ chá»n) Äáº©y code lÃªn GitHub

```bash
gh auth login
gh repo create crawl-topcv-jobs --private --source=. --remote=origin --push
```

### BÆ°á»›c 3 â€” (Tuá»³ chá»n) CÃ i Chrome cho Selenium

> KhÃ´ng báº¯t buá»™c náº¿u báº¡n chá»‰ dÃ¹ng `requests` + `BeautifulSoup`.

```bash
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt install ./google-chrome-stable_current_amd64.deb
```

---

## â–¶ï¸ Cháº¡y crawler

Script máº·c Ä‘á»‹nh crawl trang:

```
https://www.topcv.vn/tim-viec-lam-data-analyst?type_keyword=1&page={page}&sba=1
```

Cháº¡y:

```bash
cd scripts
uv run scrape_topcv_company.py
```

Káº¿t quáº£:

* In ra `head()` cá»§a DataFrame
* LÆ°u `topcv_data_analyst_jobs.csv` (UTF-8-SIG) trong thÆ° má»¥c hiá»‡n táº¡i.

> Muá»‘n Ä‘á»•i sá»‘ trang, sá»­a nhanh trong `__main__`:

```python
qtpl = "https://www.topcv.vn/tim-viec-lam-data-analyst?type_keyword=1&page={page}&sba=1"
df = crawl_to_dataframe(qtpl, start_page=1, end_page=10, delay_between_pages=(0.5, 1.0))
```

---

## âš™ï¸ Äiá»u chá»‰nh tá»‘c Ä‘á»™ (Ä‘á»ƒ trÃ¡nh 429)

Script cÃ³ hai â€œÄ‘iá»ƒm nghá»‰â€:

* `smart_sleep(0.5, 1.0)` â†’ nghá»‰ ngáº«u nhiÃªn **0.5â€“1.0s** giá»¯a cÃ¡c request (job/company).
* `delay_between_pages=(0.5, 1.0)` â†’ nghá»‰ **0.5â€“1.0s** giá»¯a cÃ¡c **trang search**.

> Náº¿u váº«n bá»‹ 429:

* TÄƒng delay (vÃ­ dá»¥ `smart_sleep(0.5, 1.0)`, `delay_between_pages=(0.5, 1.0)`).
* Thu háº¹p sá»‘ trang; cháº¡y ngoÃ i giá» cao Ä‘iá»ƒm; dÃ¹ng IP/proxy há»£p lá»‡.
* TÃ¡i sá»­ dá»¥ng `requests.Session()` (Ä‘Ã£ lÃ m sáºµn) Ä‘á»ƒ giá»¯ cookie.
* TÃ´n trá»ng robots & Ä‘iá»u khoáº£n website.

---

## ğŸ§ª Máº«u sá»­ dá»¥ng DataFrame

Má»Ÿ rá»™ng trong notebook hoáº·c script khÃ¡c:

```python
import pandas as pd

df = pd.read_csv("topcv_data_analyst_jobs.csv")
print(df.shape)
print(df.columns)

# Lá»c job á»Ÿ TP.HCM, cÃ³ â€œPythonâ€ trong mÃ´ táº£ yÃªu cáº§u
mask_hcm = df["detail_location"].fillna("").str.contains("Há»“ ChÃ­ Minh", case=False)
mask_py  = df["desc_yeucau"].fillna("").str.contains("Python", case=False)
df_filtered = df[mask_hcm & mask_py]
df_filtered.head()
```

Xuáº¥t XLSX:

```python
df.to_excel("topcv_data_analyst_jobs.xlsx", index=False)
```

---

## ğŸ§© CÃ¡c cá»™t chÃ­nh

* **Job (search):** `title`, `job_url`, `company`, `company_url`, `salary_list`, `address_list`, `exp_list`
* **Job (detail):** `detail_title`, `detail_salary`, `detail_location`, `detail_experience`, `deadline`, `tags`,
  `desc_mota`, `desc_yeucau`, `desc_quyenloi`, `working_addresses`, `working_times`, `company_url_from_job`
* **Company:** `company_name_full`, `company_website`, `company_size`, `company_industry`, `company_address`, `company_description`

> `company_url_from_job` cÃ³ thá»ƒ khÃ¡c `company_url` (Æ°u tiÃªn link trÃªn trang job náº¿u cÃ³).

---

## ğŸ§° Troubleshooting

* **429 Too Many Requests**
  TÄƒng delay (xem pháº§n â€œÄiá»u chá»‰nh tá»‘c Ä‘á»™â€), giáº£m sá»‘ trang, cháº¡y cháº­m láº¡i.

* **HTML thay Ä‘á»•i**
  DOM cá»§a TopCV cÃ³ thá»ƒ update. Æ¯u tiÃªn cÃ¡c selector â€œdá»±a trÃªn tiÃªu Ä‘á»â€ (vÃ­ dá»¥ `Má»©c lÆ°Æ¡ng`, `Äá»‹a Ä‘iá»ƒm`, `Kinh nghiá»‡m`) Ä‘á»ƒ giáº£m rá»§i ro vá»¡.

* **Selenium & Chrome**
  Chá»‰ cáº§n náº¿u báº¡n muá»‘n báº¯t network nÃ¢ng cao hoáº·c trang render máº¡nh báº±ng JS. Script hiá»‡n táº¡i khÃ´ng phá»¥ thuá»™c Selenium.

---

## ğŸ—ºï¸ Lá»™ trÃ¬nh má»Ÿ rá»™ng

* ThÃªm phÃ¢n loáº¡i keyword (Python/SQL/BI, v.v.) tá»« mÃ´ táº£ yÃªu cáº§u.
* Chuáº©n hÃ³a Ä‘á»‹a Ä‘iá»ƒm/tá»‰nh thÃ nh, chuáº©n hoÃ¡ má»©c lÆ°Æ¡ng.
* LÆ°u DB (SQLite/Postgres) + incremental update theo `job_id`.
* ThÃªm CLI flag: `--start-page`, `--end-page`, `--delay-min`, `--delay-max`, `--query`.

---

## ğŸ“„ License

MIT â€” dÃ¹ng thoáº£i mÃ¡i cho má»¥c Ä‘Ã­ch há»c táº­p & nghiÃªn cá»©u.
**LÆ°u Ã½:** HÃ£y tÃ´n trá»ng Ä‘iá»u khoáº£n sá»­ dá»¥ng cá»§a TopCV vÃ  khÃ´ng gÃ¢y táº£i lá»›n lÃªn há»‡ thá»‘ng.

---
